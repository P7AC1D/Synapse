#!/usr/bin/env python3
"""
Overfitting Fix Implementation

This script implements the critical fixes identified in the generalization analysis:
1. Stronger regularization through model configuration
2. Validation-based model selection
3. Early stopping based on validation performance
4. Improved data splitting
"""

import os
import sys
import json
import numpy as np
from pathlib import Path

# Add src to path for imports
sys.path.append(str(Path(__file__).parent / "src"))

def create_regularized_training_config():
    """Create improved training configuration with stronger regularization"""
    
    config = {
        "model_config": {
            # Stronger regularization through model architecture
            "policy_kwargs": {
                "dropout": 0.3,  # Add dropout for regularization
                "net_arch": [
                    dict(pi=[128, 128], vf=[128, 128])  # Reduced from [256, 256]
                ],
                "activation_fn": "relu",  # Explicit activation
                "ortho_init": True,  # Orthogonal initialization
            },
            
            # Learning parameters with regularization
            "learning_rate": 0.0005,  # Reduced from 0.001
            "clip_range": 0.1,        # Reduced from 0.2
            "clip_range_vf": 0.1,     # Value function clipping
            "max_grad_norm": 0.5,     # Gradient clipping
            
            # Training stability
            "batch_size": 64,         # Smaller batches
            "n_epochs": 4,            # Fewer epochs per update
            "gae_lambda": 0.9,        # Reduced GAE lambda
            
            # Exploration
            "ent_coef": 0.01,         # Increased entropy coefficient
        },
        
        "data_config": {
            # Improved data splitting
            "train_split": 0.7,       # 70% training (reduced from ~90%)
            "validation_split": 0.2,  # 20% validation (increased from ~10%)
            "test_split": 0.1,        # 10% test set
            
            # Data augmentation
            "add_noise": True,
            "noise_std": 0.01,        # 1% noise during training
            
            # Temporal validation
            "temporal_split": True,   # Maintain time order
        },
        
        "training_config": {
            # Early stopping
            "early_stopping": {
                "enabled": True,
                "patience": 10,           # Stop if no improvement for 10 iterations
                "min_improvement": 0.01,  # Minimum 1% improvement required
                "metric": "validation_return",
                "mode": "max"
            },
            
            # Model selection
            "model_selection": {
                "criterion": "validation_return",  # Use validation instead of combined
                "save_best_only": True,
                "save_frequency": 1,
            },
            
            # Monitoring
            "validation_frequency": 1,  # Validate every iteration
            "log_frequency": 1,
        }
    }
    
    return config

def create_validation_early_stopping_callback():
    """Create early stopping callback based on validation performance"""
    
    callback_code = '''
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np

class ValidationEarlyStoppingCallback(BaseCallback):
    """
    Early stopping callback based on validation performance
    """
    def __init__(self, 
                 patience=10, 
                 min_improvement=0.01,
                 verbose=1):
        super(ValidationEarlyStoppingCallback, self).__init__(verbose)
        self.patience = patience
        self.min_improvement = min_improvement
        self.best_val_return = -np.inf
        self.wait = 0
        self.stopped_iteration = 0
        
    def _on_step(self) -> bool:
        return True
    
    def on_validation_end(self, val_return, iteration):
        """Called after validation evaluation"""
        
        if val_return > self.best_val_return + self.min_improvement:
            if self.verbose >= 1:
                print(f"Validation improvement: {val_return:.3f} > {self.best_val_return:.3f}")
            self.best_val_return = val_return
            self.wait = 0
        else:
            self.wait += 1
            if self.verbose >= 1:
                print(f"No improvement for {self.wait}/{self.patience} iterations")
        
        if self.wait >= self.patience:
            if self.verbose >= 1:
                print(f"Early stopping triggered at iteration {iteration}")
                print(f"Best validation return: {self.best_val_return:.3f}")
            self.stopped_iteration = iteration
            return False  # Stop training
        
        return True  # Continue training
'''
    
    return callback_code

def create_regularized_model_config():
    """Create model configuration with stronger regularization"""
    
    model_code = '''
from stable_baselines3 import PPO
from stable_baselines3.common.policies import ActorCriticPolicy
import torch.nn as nn

class RegularizedPolicy(ActorCriticPolicy):
    """Custom policy with regularization"""
    
    def __init__(self, *args, **kwargs):
        # Add dropout and regularization
        kwargs['net_arch'] = [dict(pi=[128, 128], vf=[128, 128])]
        kwargs['activation_fn'] = nn.ReLU
        kwargs['ortho_init'] = True
        super(RegularizedPolicy, self).__init__(*args, **kwargs)
    
    def forward(self, obs, deterministic=False):
        """Forward pass with noise injection during training"""
        
        # Add noise during training for regularization
        if self.training and not deterministic:
            noise = torch.randn_like(obs) * 0.01
            obs = obs + noise
        
        return super().forward(obs, deterministic)

def create_regularized_model(env, seed=42):
    """Create PPO model with regularization"""
    
    model = PPO(
        RegularizedPolicy,
        env,
        
        # Learning parameters
        learning_rate=0.0005,
        n_steps=2048,
        batch_size=64,
        n_epochs=4,
        gamma=0.99,
        gae_lambda=0.9,
        
        # Clipping
        clip_range=0.1,
        clip_range_vf=0.1,
        max_grad_norm=0.5,
        
        # Regularization
        ent_coef=0.01,
        vf_coef=0.5,
        
        # Logging
        verbose=1,
        seed=seed,
        
        # Device
        device='auto'
    )
    
    return model
'''
    
    return model_code

def create_improved_data_splitter():
    """Create improved data splitting with proper validation set"""
    
    splitter_code = '''
import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit

class ImprovedDataSplitter:
    """
    Improved data splitter with proper validation and temporal awareness
    """
    
    def __init__(self, 
                 train_ratio=0.7, 
                 val_ratio=0.2, 
                 test_ratio=0.1,
                 temporal_split=True):
        
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6
        
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        self.temporal_split = temporal_split
    
    def split_data(self, data):
        """
        Split data into train/validation/test sets
        
        Args:
            data: DataFrame with temporal data
            
        Returns:
            tuple: (train_data, val_data, test_data)
        """
        
        n_samples = len(data)
        
        if self.temporal_split:
            # Temporal split - maintain time order
            train_end = int(n_samples * self.train_ratio)
            val_end = int(n_samples * (self.train_ratio + self.val_ratio))
            
            train_data = data.iloc[:train_end]
            val_data = data.iloc[train_end:val_end]
            test_data = data.iloc[val_end:]
            
        else:
            # Random split
            indices = np.random.permutation(n_samples)
            train_end = int(n_samples * self.train_ratio)
            val_end = int(n_samples * (self.train_ratio + self.val_ratio))
            
            train_indices = indices[:train_end]
            val_indices = indices[train_end:val_end]
            test_indices = indices[val_end:]
            
            train_data = data.iloc[train_indices]
            val_data = data.iloc[val_indices]
            test_data = data.iloc[test_indices]
        
        print(f"Data split - Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}")
        print(f"Ratios - Train: {len(train_data)/n_samples:.2%}, Val: {len(val_data)/n_samples:.2%}, Test: {len(test_data)/n_samples:.2%}")
        
        return train_data, val_data, test_data
    
    def cross_validation_split(self, data, n_splits=5):
        """
        Create time series cross-validation splits
        """
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        splits = []
        
        for train_idx, val_idx in tscv.split(data):
            train_data = data.iloc[train_idx]
            val_data = data.iloc[val_idx]
            splits.append((train_data, val_data))
        
        return splits
'''
    
    return splitter_code

def save_implementation_files():
    """Save all implementation files"""
    
    src_dir = Path("src")
    callbacks_dir = src_dir / "callbacks"
    utils_dir = src_dir / "utils"
    
    # Create directories if they don't exist
    callbacks_dir.mkdir(exist_ok=True)
    utils_dir.mkdir(exist_ok=True)
    
    # Save configuration
    config = create_regularized_training_config()
    with open("regularized_training_config.json", 'w') as f:
        json.dump(config, f, indent=2)
    
    # Save early stopping callback
    callback_code = create_validation_early_stopping_callback()
    with open(callbacks_dir / "validation_early_stopping.py", 'w') as f:
        f.write(callback_code)
    
    # Save regularized model
    model_code = create_regularized_model_config()
    with open(src_dir / "regularized_model.py", 'w') as f:
        f.write(model_code)
    
    # Save data splitter
    splitter_code = create_improved_data_splitter()
    with open(utils_dir / "improved_data_splitter.py", 'w') as f:
        f.write(splitter_code)
    
    print("âœ… Implementation files created:")
    print(f"   - regularized_training_config.json")
    print(f"   - {callbacks_dir}/validation_early_stopping.py")
    print(f"   - {src_dir}/regularized_model.py")
    print(f"   - {utils_dir}/improved_data_splitter.py")

def create_training_script():
    """Create updated training script with regularization fixes"""
    
    script_code = '''#!/usr/bin/env python3
"""
Regularized Training Script

This script implements the overfitting fixes:
- Stronger regularization
- Validation-based model selection
- Early stopping
- Improved data splitting
"""

import json
import sys
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent / "src"))

from regularized_model import create_regularized_model
from callbacks.validation_early_stopping import ValidationEarlyStoppingCallback
from utils.improved_data_splitter import ImprovedDataSplitter
from callbacks.anti_collapse_callback import AntiCollapseCallback
from callbacks.eval_callback import EvalCallback

def train_regularized_model(config_path="regularized_training_config.json", seed=1006):
    """Train model with regularization fixes"""
    
    # Load configuration
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    print("ðŸš€ Starting regularized training with overfitting fixes...")
    print(f"   Seed: {seed}")
    print(f"   Config: {config_path}")
    
    # TODO: Implement full training pipeline with:
    # 1. Improved data splitting (70/20/10)
    # 2. Regularized model creation
    # 3. Validation-based callbacks
    # 4. Early stopping
    # 5. Enhanced monitoring
    
    print("âœ… Training configuration ready for implementation")
    print("   Next: Integrate with existing training pipeline")

if __name__ == "__main__":
    train_regularized_model()
'''
    
    with open("train_regularized.py", 'w') as f:
        f.write(script_code)
    
    print(f"   - train_regularized.py")

def main():
    """Main implementation function"""
    
    print("ðŸ”§ Creating overfitting fix implementation files...")
    
    # Save all implementation files
    save_implementation_files()
    create_training_script()
    
    print("\nðŸ“‹ IMPLEMENTATION CHECKLIST:")
    print("âœ… 1. Regularized model configuration created")
    print("âœ… 2. Validation early stopping callback created")
    print("âœ… 3. Improved data splitter created")
    print("âœ… 4. Training configuration with fixes created")
    print("âœ… 5. Updated training script template created")
    
    print("\nðŸŽ¯ NEXT STEPS:")
    print("1. Review generated configurations")
    print("2. Integrate with existing training pipeline")
    print("3. Test on small dataset first")
    print("4. Run full training with validation monitoring")
    print("5. Verify improved generalization")
    
    print(f"\nðŸ“ Files created in: {Path.cwd()}")

if __name__ == "__main__":
    main()
'''

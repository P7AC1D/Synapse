"""
NO EARLY STOPPING Training utilities for PPO-LSTM model with walk-forward optimization.

This module is a modified version of training_utils_optimized_enhanced.py with ALL
early stopping mechanisms removed to allow full WFO cycles to complete.

REMOVED:
- ValidationAwareEarlyStoppingCallback
- TradingAwareEarlyStoppingCallback  
- All early stopping checks and evaluations
- Overfitting detection mechanisms

PRESERVED:
- All optimization features (5-10x speedup)
- Model evaluation and selection
- Warm starting
- Environment caching
- Progressive hyperparameters

This ensures WFO training completes full cycles for volatile financial markets
where early stopping interferes with learning patterns.
"""
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Tuple, Dict, Any, List, Optional
import time
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.utils import get_linear_fn
from sb3_contrib.ppo_recurrent import RecurrentPPO
from trading.environment import TradingEnv
import torch as th

from callbacks.epsilon_callback import CustomEpsilonCallback
from callbacks.eval_callback import UnifiedEvalCallback

# Import optimization utilities but exclude early stopping
from utils.training_utils_optimized_enhanced import (
    get_enhanced_hyperparameters,
    TRAINING_SCHEDULES_ENHANCED,
    MODEL_KWARGS_ANTI_OVERFITTING,
    EnvironmentCache,
    format_time_remaining,
    calculate_adaptive_timesteps,
    save_training_state,
    load_training_state,
    create_optimized_environment,
    clear_optimization_cache,
    get_optimization_info,
    FAST_EVALUATION_AVAILABLE
)

# Import the environment cache instance
from utils.training_utils_optimized import _env_cache

def train_walk_forward_no_early_stopping(data: pd.DataFrame, initial_window: int, step_size: int, args) -> RecurrentPPO:
    """
    Walk-forward training with NO EARLY STOPPING to allow full WFO cycles.
    
    This function removes all early stopping mechanisms while preserving optimizations:
    - Adaptive timestep reduction (2-4x speedup)
    - Warm-starting between iterations (1.5-2x speedup)  
    - Progressive hyperparameter scheduling (1.2-1.5x speedup)
    - Environment preprocessing cache (1.3-1.5x speedup)
    - Optimized evaluation frequency (1.2-1.3x speedup)
    
    REMOVED: Early stopping, overfitting detection, validation gap monitoring
    PRESERVED: All performance optimizations and model selection
    
    Args:
        data: Full dataset for training
        initial_window: Size of initial training window
        step_size: Step size for moving window forward
        args: Training arguments
        
    Returns:
        RecurrentPPO: Final trained model after full WFO completion
    """
    total_periods = len(data)
    base_timesteps = getattr(args, 'total_timesteps', 50000)
    
    # Calculate total number of iterations
    total_iterations = (total_periods - initial_window) // step_size + 1
    
    # Initialize optimization tracking (NO early stopping stats)
    optimization_stats = {
        'total_speedup_achieved': 0.0,
        'avg_iteration_time': 0.0,
        'total_timesteps_saved': 0,
        'warm_starts': 0,
        'cache_hits': 0,
        'validation_improvements': 0,
        'max_train_val_gap': 0.0,
        'full_wfo_completion': True  # New flag to indicate full completion
    }
    
    # Track validation performance for hyperparameter adjustment (but no early stopping)
    validation_performance_history = []
    
    # NO EARLY STOPPING INITIALIZATION - removed ValidationAwareEarlyStoppingCallback
    print(f"\nüö´ EARLY STOPPING DISABLED - Full WFO completion guaranteed")
    print(f"üéØ Will complete all {total_iterations} iterations regardless of performance")
    
    state_path = f"../results/{args.seed}/training_state_no_early_stopping.json"
    training_start, _, state = load_training_state(state_path)
    
    best_model_path = os.path.join(f"../results/{args.seed}", "best_model_no_early_stopping.zip")
    if os.path.exists(best_model_path) and getattr(args, 'warm_start', True):
        print(f"üîÑ Resuming NO EARLY STOPPING training from step {training_start}")
        model = RecurrentPPO.load(best_model_path)
        optimization_stats['warm_starts'] += 1
    else:
        print("üöÄ Starting new NO EARLY STOPPING training")
        training_start = 0
        model = None
    
    print(f"\nüõ°Ô∏è NO EARLY STOPPING FEATURES:")
    print(f"Early Stopping: ‚ùå DISABLED (allows full WFO cycles)")
    print(f"Validation Gap Monitoring: ‚ùå DISABLED")
    print(f"Overfitting Detection: ‚ùå DISABLED")
    print(f"Regularization: ‚úì (preserved from enhanced version)")
    print(f"Conservative Training: ‚úì (preserved)")
    print(f"Adaptive Timesteps: {'‚úì' if getattr(args, 'adaptive_timesteps', True) else '‚úó'}")
    print(f"Warm Starting: ‚úì")
    print(f"Environment Caching: ‚úì")
    
    # Enable fast evaluation by default if available
    use_fast_evaluation = getattr(args, 'use_fast_evaluation', True) and FAST_EVALUATION_AVAILABLE
    if use_fast_evaluation:
        print("üöÄ Using OPTIMIZED evaluation for model comparison")
    else:
        print("‚ö† Using standard evaluation")
      # Progress tracking
    try:
        print(f"\nüéØ Starting NO EARLY STOPPING walk-forward training...")
        print(f"üìä Target iterations: {total_iterations} (ALL will complete)")
        print(f"‚è±Ô∏è Estimated time: {(total_iterations - training_start) * 45 / 60:.1f} hours")
        
        iteration_times = []
        
        for iteration in range(training_start, total_iterations):
            iteration_start_time = time.time()
            
            # Calculate data splits
            val_size = min(step_size, 2000)  # Limit validation size for speed
            train_size = initial_window - val_size
            
            train_start = iteration * step_size
            train_end = train_start + train_size
            val_end = min(train_end + val_size, total_periods)
            
            # Ensure we have enough validation data
            if val_end - train_end < val_size * 0.3:
                print(f"\n‚ö†Ô∏è Insufficient validation data at iteration {iteration + 1}, stopping")
                break
                
            train_data = data.iloc[train_start:train_end].copy()
            val_data = data.iloc[train_end:val_end].copy()
        
            train_data.index = data.index[train_start:train_end]
            val_data.index = data.index[train_end:val_end]
            
            print(f"\n=== NO EARLY STOPPING Training Period: {train_data.index[0]} to {train_data.index[-1]} ===")
            print(f"=== Validation Period: {val_data.index[0]} to {val_data.index[-1]} ===")
            print(f"=== Iteration: {iteration+1}/{total_iterations} (FULL WFO COMPLETION) ===")
        
            # Environment parameters
            env_params = {
                'initial_balance': getattr(args, 'initial_balance', 10000),
                'balance_per_lot': getattr(args, 'balance_per_lot', 500),
                'random_start': getattr(args, 'random_start', False),
                'point_value': getattr(args, 'point_value', 0.01),
                'min_lots': getattr(args, 'min_lots', 0.01),
                'max_lots': getattr(args, 'max_lots', 1.0),
                'contract_size': getattr(args, 'contract_size', 100000)
            }
        
            # Create optimized environments with caching
            if getattr(args, 'cache_environments', True):
                train_cache_key = _env_cache.get_cache_key(train_data, env_params)
                val_cache_key = _env_cache.get_cache_key(val_data, {**env_params, 'random_start': False})
                
                train_env = Monitor(create_optimized_environment(
                    train_data, env_params, train_cache_key, True
                ))
                val_env = Monitor(create_optimized_environment(
                    val_data, {**env_params, 'random_start': False}, val_cache_key, True
                ))
                optimization_stats['cache_hits'] += 2
            else:
                train_env = Monitor(TradingEnv(train_data, **env_params))
                val_env = Monitor(TradingEnv(val_data, **{**env_params, 'random_start': False}))
              # Get enhanced hyperparameters (conservative for anti-overfitting)
            enhanced_params = get_enhanced_hyperparameters(iteration, total_iterations, validation_performance_history)
            
            # Determine which schedule was selected based on the parameters
            schedule_name = "conservative"  # default
            if enhanced_params.get('learning_rate', 0) >= 8e-4:
                schedule_name = "aggressive"
            elif enhanced_params.get('learning_rate', 0) >= 5e-4:
                schedule_name = "balanced"
            
            print(f"üìà Enhanced training schedule: {schedule_name}")              # Calculate adaptive timesteps
            if getattr(args, 'adaptive_timesteps', True):
                current_timesteps = calculate_adaptive_timesteps(
                    iteration, base_timesteps, 1000
                )
                if current_timesteps != base_timesteps:
                    optimization_stats['total_timesteps_saved'] += (base_timesteps - current_timesteps)
                    print(f"‚ö° Adaptive timesteps: {current_timesteps:,} (saved {base_timesteps - current_timesteps:,})")
            else:
                current_timesteps = base_timesteps
            
            if model is None:
                print("\nüöÄ Performing ENHANCED initial training (NO EARLY STOPPING)...")
                
                # Merge enhanced hyperparameters directly (they're a flat dict)
                model_kwargs = {**MODEL_KWARGS_ANTI_OVERFITTING, **enhanced_params}
                
                # Initialize new model
                model = RecurrentPPO(
                    "MlpLstmPolicy",
                    train_env,
                    policy_kwargs={
                        'net_arch': [256, 256],  # Enhanced architecture
                        'lstm_hidden_size': 256,
                        'n_lstm_layers': 2,
                        'shared_lstm': False,
                        'enable_critic_lstm': True,
                        'lstm_kwargs': {'dropout': 0.1}  # Regularization
                    },
                    verbose=0,
                    device=getattr(args, 'device', 'auto'),
                    seed=getattr(args, 'seed', None),
                    **model_kwargs
                )
                
                # Set up callbacks (NO early stopping)
                callbacks = [
                    # Enhanced exploration 
                    CustomEpsilonCallback(
                        start_eps=0.5,  # Moderate exploration
                        end_eps=0.05,   # Maintain exploration
                        decay_timesteps=int(current_timesteps * 0.7),
                        iteration=iteration
                    ),
                    # Evaluation callback (for model selection only, no early stopping)
                    UnifiedEvalCallback(
                        val_env,
                        train_data=train_data,
                        val_data=val_data,
                        best_model_save_path=f"../results/{args.seed}",
                        log_path=f"../results/{args.seed}",
                        eval_freq=getattr(args, 'eval_freq', 5000),
                        deterministic=True,
                        verbose=1,
                        iteration=iteration,
                        training_timesteps=current_timesteps
                    )
                ]
                
                # Train initial model
                model.learn(
                    total_timesteps=current_timesteps,
                    callback=callbacks,
                    progress_bar=True,
                    reset_num_timesteps=True
                )
                
            else:
                print(f"\n‚ö° Continuing ENHANCED training with warm start (NO EARLY STOPPING)...")
                print(f"Training timesteps: {current_timesteps:,}")
                  # Update model environment
                model.set_env(train_env)
                
                # Apply enhanced hyperparameters directly (they're a flat dict)
                for param, value in enhanced_params.items():
                    if hasattr(model, param):
                        setattr(model, param, value)
                        print(f"üìà Updated {param} = {value}")
                
                # Set up callbacks for continued training (NO early stopping)
                callbacks = [
                    CustomEpsilonCallback(
                        start_eps=0.25 if iteration < 5 else 0.15,
                        end_eps=0.05,
                        decay_timesteps=int(current_timesteps * 0.7),
                        iteration=iteration
                    ),
                    UnifiedEvalCallback(
                        val_env,
                        train_data=train_data,
                        val_data=val_data,
                        best_model_save_path=f"../results/{args.seed}",
                        log_path=f"../results/{args.seed}",
                        eval_freq=getattr(args, 'eval_freq', 5000),
                        deterministic=True,
                        verbose=0,
                        iteration=iteration,
                        training_timesteps=current_timesteps
                    )
                ]
                
                # Perform training
                model.learn(
                    total_timesteps=current_timesteps,
                    callback=callbacks,
                    progress_bar=True,
                    reset_num_timesteps=True
                )
                
                optimization_stats['warm_starts'] += 1
            
            # Model evaluation for tracking (but no early stopping decisions)
            training_results = None
            validation_results = None
            
            try:
                # Evaluate on training data
                if use_fast_evaluation:
                    from utils.training_utils import evaluate_model_on_dataset
                    training_results = evaluate_model_on_dataset(
                        f"../results/{args.seed}/curr_best_model.zip", 
                        train_data, args, use_fast_evaluation=True
                    )
                    validation_results = evaluate_model_on_dataset(
                        f"../results/{args.seed}/curr_best_model.zip", 
                        val_data, args, use_fast_evaluation=True
                    )
                else:
                    print("üìä Skipping detailed evaluation (fast evaluation not available)")
            except Exception as e:
                print(f"‚ö†Ô∏è Evaluation error (non-critical): {e}")
            
            # Track performance for hyperparameter adjustment (no early stopping)
            if validation_results:
                validation_score = validation_results.get('score', 0)
                validation_performance_history.append(validation_score)
                
                if training_results:
                    training_score = training_results.get('score', 0)
                    gap = abs(training_score - validation_score) / max(abs(training_score), 1e-6)
                    optimization_stats['max_train_val_gap'] = max(optimization_stats['max_train_val_gap'], gap)
                    
                    print(f"\nüìä PERFORMANCE TRACKING (NO EARLY STOPPING):")
                    print(f"   Training Score: {training_score:.4f}")
                    print(f"   Validation Score: {validation_score:.4f}")
                    print(f"   Performance Gap: {gap:.1%}")
                    print(f"   ‚úÖ Continuing training regardless of performance gap")
            
            # Model selection based on validation performance (preserve best model)
            curr_best_path = os.path.join(f"../results/{args.seed}", "curr_best_model.zip")
            
            if os.path.exists(curr_best_path):
                # Load the validation-selected current best model
                curr_best_metrics_path = curr_best_path.replace(".zip", "_metrics.json")
                
                if os.path.exists(best_model_path):
                    # Compare validation scores for model selection (no early stopping)
                    try:
                        with open(curr_best_metrics_path, 'r') as f:
                            curr_metrics = json.load(f)
                        
                        best_metrics_path = best_model_path.replace(".zip", "_metrics.json")
                        if os.path.exists(best_metrics_path):
                            with open(best_metrics_path, 'r') as f:
                                best_metrics = json.load(f)
                            
                            curr_score = curr_metrics.get('validation_score', curr_metrics.get('enhanced_score', 0))
                            best_score = best_metrics.get('validation_score', best_metrics.get('enhanced_score', 0))
                            
                            if curr_score > best_score:
                                model = RecurrentPPO.load(curr_best_path)
                                model.save(best_model_path)
                                import shutil
                                shutil.copy2(curr_best_metrics_path, best_metrics_path)
                                optimization_stats['validation_improvements'] += 1
                                print(f"\nüéØ New best model: validation score {curr_score:.4f} > {best_score:.4f}")
                            else:
                                model = RecurrentPPO.load(best_model_path)
                                print(f"\nüìä Keeping previous best model: validation score {best_score:.4f} >= {curr_score:.4f}")
                        else:
                            model = RecurrentPPO.load(curr_best_path)
                            model.save(best_model_path)
                            import shutil
                            shutil.copy2(curr_best_metrics_path, best_metrics_path)
                            print(f"\nüéØ First model saved as best")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Model comparison error: {e}")
                        model = RecurrentPPO.load(curr_best_path)
                        model.save(best_model_path)
                
                # Clean up current model files
                os.remove(curr_best_path)
                if os.path.exists(curr_best_metrics_path):
                    os.remove(curr_best_metrics_path)
            
            # Save training state
            iteration_time = time.time() - iteration_start_time
            iteration_times.append(iteration_time)
            optimization_stats['avg_iteration_time'] = sum(iteration_times) / len(iteration_times)
            
            # Calculate achieved speedup
            baseline_time = 45 * 60  # 45 minutes baseline
            current_speedup = baseline_time / iteration_time
            optimization_stats['total_speedup_achieved'] = current_speedup
            
            print(f"\n‚ö° OPTIMIZATION PERFORMANCE:")
            print(f"Iteration time: {iteration_time/60:.1f} minutes")
            print(f"Achieved speedup: {current_speedup:.1f}x vs original")
            print(f"Progress: {iteration+1}/{total_iterations} ({(iteration+1)/total_iterations*100:.1f}%)")
            print(f"‚úÖ NO EARLY STOPPING - continuing to next iteration")
              # Save state for resumption
            save_training_state({
                'iteration': iteration + 1,
                'validation_scores': validation_performance_history,
                'model_path': best_model_path,
                'optimization_stats': optimization_stats,
                'no_early_stopping': True
            }, state_path)
            
            # Update estimated time remaining (removed custom progress indicator)
            if len(iteration_times) > 0:
                remaining_iterations = total_iterations - iteration - 1
                avg_time = sum(iteration_times) / len(iteration_times)
                estimated_remaining = remaining_iterations * avg_time
                if remaining_iterations > 0:
                    print(f"‚è±Ô∏è Estimated time remaining: {estimated_remaining/3600:.1f} hours")

    except KeyboardInterrupt:
        print("\nüõë Training interrupted. Progress saved - use same command to resume.")
        return model
    finally:
        clear_optimization_cache()

    # Load final model
    if os.path.exists(best_model_path):
        model = RecurrentPPO.load(best_model_path)

    # Save final summary
    final_summary = {
        'no_early_stopping_completion': True,
        'total_iterations_completed': total_iterations,
        'total_speedup_achieved': optimization_stats['total_speedup_achieved'],
        'total_timesteps_saved': optimization_stats['total_timesteps_saved'],
        'warm_starts': optimization_stats['warm_starts'],
        'cache_hits': optimization_stats['cache_hits'],
        'validation_improvements': optimization_stats['validation_improvements'],
        'max_train_val_gap': optimization_stats['max_train_val_gap'],
        'avg_iteration_time_minutes': optimization_stats['avg_iteration_time'] / 60,
        'estimated_time_saved_hours': (45 - optimization_stats['avg_iteration_time'] / 60) * total_iterations / 60,
        'completion_timestamp': datetime.now().isoformat()
    }
    
    with open(f"../results/{args.seed}/no_early_stopping_summary.json", 'w') as f:
        json.dump(final_summary, f, indent=2)
    
    print(f"\nüéâ NO EARLY STOPPING TRAINING COMPLETED!")
    print(f"‚úÖ Full WFO cycles completed: {total_iterations}/{total_iterations}")
    print(f"‚ö° Total speedup achieved: {optimization_stats['total_speedup_achieved']:.1f}x")
    print(f"üíæ Total timesteps saved: {optimization_stats['total_timesteps_saved']:,}")
    print(f"üîÑ Warm starts used: {optimization_stats['warm_starts']}")
    print(f"‚è±Ô∏è Average iteration time: {optimization_stats['avg_iteration_time']/60:.1f} minutes")
    print(f"üö´ Early stops triggered: 0 (DISABLED)")
    print(f"üìà Model improvements: {optimization_stats['validation_improvements']}")
    
    return model
